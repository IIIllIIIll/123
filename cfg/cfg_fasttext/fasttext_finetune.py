#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""CFG FastTextæ¨¡å‹è®­ç»ƒå·¥å…·

ä½¿ç”¨FastText CLIè¿›è¡ŒåŸºäºé¢„è®­ç»ƒè¯å‘é‡çš„CFGæ“ä½œç æ¨¡å‹è®­ç»ƒã€‚

ä¸»è¦åŠŸèƒ½:
1. CFGæ“ä½œç æ•°æ®é¢„å¤„ç†å’Œåˆ†å‰²
2. åŸºäºé¢„è®­ç»ƒè¯å‘é‡çš„FastTextæ¨¡å‹è®­ç»ƒ
3. æ¨¡å‹è¯„ä¼°å’Œæ€§èƒ½åˆ†æ
4. è®­ç»ƒæŠ¥å‘Šç”Ÿæˆ

ä½¿ç”¨æ–¹å¼:
    python fasttext_finetune.py --pretrained-model cc.en.300.vec --corpus cfg_corpus.txt --output-dir ./output

ä¾èµ–:
    - FastText C++å‘½ä»¤è¡Œå·¥å…·
    - Python fasttextåº“
    - é¢„è®­ç»ƒå‘é‡æ–‡ä»¶ï¼ˆå¦‚cc.en.300.vecï¼‰
"""

import os
import sys
import argparse
import logging
import subprocess
from typing import List, Tuple, Optional
import fasttext
import numpy as np
from sklearn.model_selection import train_test_split


class CFGFastTextFineTuner:
    """CFG FastTextæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, pretrained_model_path: str, corpus_path: str, output_dir: str, model_dim: int):
        """åˆå§‹åŒ–CFG FastTextè®­ç»ƒå™¨"""
        self.pretrained_model_path = pretrained_model_path
        self.corpus_path = corpus_path
        self.output_dir = output_dir
        self.model_dim = model_dim
        
        os.makedirs(output_dir, exist_ok=True)
        self.setup_logging()
        self.model = None
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        log_file = os.path.join(self.output_dir, 'cfg_fasttext_finetune.log')
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def preprocess_corpus(self) -> Tuple[List[str], int]:
        """é¢„å¤„ç†CFGæ“ä½œç è¯­æ–™åº“æ•°æ®"""
        try:
            self.logger.info(f"é¢„å¤„ç†CFGæ“ä½œç è¯­æ–™åº“: {self.corpus_path}")
            
            processed_texts = []
            total_lines = 0
            
            with open(self.corpus_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # è¿‡æ»¤è¿‡çŸ­çš„æ“ä½œç 
                    opcodes = [opcode for opcode in line.split() if len(opcode) >= 2]
                    if opcodes:
                        processed_texts.append(' '.join(opcodes))
                    total_lines += 1
            
            self.logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ–‡æœ¬: {len(processed_texts)} è¡Œ")
            return processed_texts, total_lines
            
        except Exception as e:
            self.logger.error(f"é¢„å¤„ç†å¤±è´¥: {e}")
            return [], 0
    
    def create_training_data(self, texts: List[str], test_size: float = 0.2) -> Tuple[str, str]:
        """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®æ–‡ä»¶"""
        try:
            # åˆ†å‰²æ•°æ®
            if len(texts) > 1:
                train_texts, val_texts = train_test_split(texts, test_size=test_size, random_state=42)
            else:
                train_texts = texts
                val_texts = texts[:1] if texts else []
            
            # åˆ›å»ºæ–‡ä»¶
            train_file = os.path.join(self.output_dir, 'train_cfg_corpus.txt')
            val_file = os.path.join(self.output_dir, 'val_cfg_corpus.txt')
            
            with open(train_file, 'w', encoding='utf-8') as f:
                for text in train_texts:
                    f.write(text + '\n')
            
            with open(val_file, 'w', encoding='utf-8') as f:
                for text in val_texts:
                    f.write(text + '\n')
            
            self.logger.info(f"è®­ç»ƒæ•°æ®: {len(train_texts)} è¡Œï¼ŒéªŒè¯æ•°æ®: {len(val_texts)} è¡Œ")
            return train_file, val_file
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºè®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            return "", ""
    
    def finetune_model(self, train_file: str, epochs: int = 5, lr: float = 0.05, min_count: int = 1) -> bool:
        """åŸºäºé¢„è®­ç»ƒè¯å‘é‡è®­ç»ƒCFG FastTextæ¨¡å‹"""
        try:
            self.logger.info("å¼€å§‹è®­ç»ƒCFG FastTextæ¨¡å‹...")
            
            # å®šä¹‰è¾“å‡ºæ¨¡å‹å‰ç¼€
            output_model_prefix = os.path.join(self.output_dir, 'cfg_finetuned_fasttext_model')
            finetuned_model_path = f"{output_model_prefix}.bin"

            # æ„å»ºCLIå‘½ä»¤
            command = [
                'fasttext', 'skipgram',
                '-pretrainedVectors', self.pretrained_model_path,
                '-dim', str(self.model_dim),
                '-output', output_model_prefix,
                '-input', train_file,
                '-epoch', str(epochs),
                '-lr', str(lr),
                '-minCount', str(min_count),
                '-thread', '4',
                '-verbose', '2'
            ]
            
            # æ‰§è¡Œå‘½ä»¤
            result = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')
            
            if result.returncode != 0:
                self.logger.error(f"FastText CLIæ‰§è¡Œå¤±è´¥: {result.stderr}")
                return False
            
            if not os.path.exists(finetuned_model_path):
                self.logger.error(f"æ¨¡å‹æ–‡ä»¶æœªç”Ÿæˆ: {finetuned_model_path}")
                return False

            # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
            self.model = fasttext.load_model(finetuned_model_path)
            self.logger.info(f"CFGæ¨¡å‹è®­ç»ƒå®Œæˆ: {finetuned_model_path}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"CFGæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False

    def evaluate_model(self, val_file: str) -> dict:
        """è¯„ä¼°CFGæ¨¡å‹æ€§èƒ½ (å¢å¼ºç‰ˆï¼šè®¡ç®—ç›¸é‚»ç›¸ä¼¼åº¦)"""
        try:
            if not self.model:
                self.logger.error("æ¨¡å‹æœªåŠ è½½")
                return {}
            
            self.logger.info("æ­£åœ¨è¯„ä¼°CFGæ¨¡å‹...")
            
            # è¯»å–éªŒè¯æ•°æ®
            with open(val_file, 'r', encoding='utf-8') as f:
                val_texts = [line.strip() for line in f if line.strip()]
            
            if not val_texts:
                self.logger.warning("éªŒè¯æ•°æ®ä¸ºç©º")
                return {}
            
            # è®¡ç®—æ“ä½œç è¦†ç›–ç‡
            total_opcodes = 0
            covered_opcodes = 0
            all_opcodes_set = set()
            
            for text in val_texts:
                opcodes = text.split()
                total_opcodes += len(opcodes)
                all_opcodes_set.update(opcodes)
                for opcode in opcodes:
                    if opcode in self.model.words:
                        covered_opcodes += 1
            
            coverage = covered_opcodes / total_opcodes if total_opcodes > 0 else 0
            
            # å…³é”®è¯„ä¼°ï¼šè®¡ç®—ç›¸é‚»æ“ä½œç çš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦
            # è¿™æ˜¯SkipGramæ¨¡å‹çš„çœŸæ­£æˆåŠŸæ ‡å‡†
            similarities = []
            # æŠ½æ ·è¯„ä¼°ï¼Œé¿å…å¤ªæ…¢
            sample_texts = val_texts[:min(500, len(val_texts))]
            
            for text in sample_texts:
                opcodes = text.split()
                if len(opcodes) >= 2:
                    for i in range(len(opcodes) - 1):
                        try:
                            # è·å–å‘é‡
                            vec1 = self.model.get_word_vector(opcodes[i])
                            vec2 = self.model.get_word_vector(opcodes[i + 1])
                            
                            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                            norm1 = np.linalg.norm(vec1)
                            norm2 = np.linalg.norm(vec2)
                            
                            if norm1 > 0 and norm2 > 0:
                                sim = np.dot(vec1, vec2) / (norm1 * norm2)
                                similarities.append(sim)
                        except:
                            # è¯ä¸åœ¨è¯æ±‡è¡¨ï¼ˆè™½ç„¶get_word_vectorèƒ½å¤„ç†OOVï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰
                            continue
            
            avg_similarity = np.mean(similarities) if similarities else 0
            
            results = {
                'vocabulary_size': len(self.model.words),
                'dimension': self.model.get_dimension(),
                'opcode_coverage_rate': coverage,
                'avg_adjacent_similarity': avg_similarity,  # å…³é”®æŒ‡æ ‡
                'similarity_samples_count': len(similarities),
                'total_opcodes_in_val': total_opcodes,
                'total_unique_opcodes_in_val': len(all_opcodes_set),
                'covered_opcodes_in_val': covered_opcodes
            }
            
            self.logger.info("CFGè¯„ä¼°ç»“æœ:")
            for key, value in results.items():
                self.logger.info(f"  {key}: {value}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"CFGè¯„ä¼°å¤±è´¥: {e}")
            return {}
    
    def test_model(self, test_opcodes: List[str] = None) -> dict:
        """æµ‹è¯•CFGæ¨¡å‹åŠŸèƒ½ (å¢å¼ºç‰ˆï¼šè·å–æœ€è¿‘é‚»)"""
        try:
            if not self.model:
                self.logger.error("æ¨¡å‹æœªåŠ è½½")
                return {}
            
            self.logger.info("æ­£åœ¨æµ‹è¯•è®­ç»ƒåçš„CFGæ¨¡å‹ (è·å–æœ€è¿‘é‚»)...")
            
            # é»˜è®¤æµ‹è¯•æ“ä½œç 
            if not test_opcodes:
                test_opcodes = [
                    'mov', 'push', 'pop', 'call', 'ret', 'jmp', 'cmp', 'test',
                    'je', 'jne', 'add', 'sub', 'xor'
                ]
            
            results = {}
            oov_count = 0
            total_test_opcodes = len(test_opcodes)
            
            for opcode in test_opcodes:
                try:
                    vector = self.model.get_word_vector(opcode)
                    similar_words = []
                    in_vocab = opcode in self.model.words
                    
                    if not in_vocab:
                        oov_count += 1
                    
                    try:
                        # FastText æ€»æ˜¯èƒ½è·å–æœ€è¿‘é‚» (å³ä½¿æ˜¯OOV)
                        neighbors = self.model.get_nearest_neighbors(opcode, k=5)
                        similar_words = [neighbor[1] for neighbor in neighbors]
                    except Exception as nn_e:
                        self.logger.debug(f"è·å–æœ€è¿‘é‚»å¤±è´¥ {opcode}: {nn_e}")
                        similar_words = ["(lookup failed)"]
                    
                    results[opcode] = {
                        'in_vocabulary': in_vocab,
                        'vector_norm': float(np.linalg.norm(vector)),
                        'nearest_neighbors': similar_words
                    }
                    
                except Exception as e:
                    self.logger.warning(f"æµ‹è¯•è¯æ±‡ '{opcode}' å¤±è´¥: {e}")
                    results[opcode] = {
                        'in_vocabulary': False,
                        'error': str(e),
                        'nearest_neighbors': []
                    }
            
            in_vocab_count = total_test_opcodes - oov_count
            
            results['_statistics'] = {
                'total_test_opcodes': total_test_opcodes,
                'in_vocab_opcodes_count': in_vocab_count,
                'oov_opcodes_count': oov_count,
                'oov_rate': oov_count / total_test_opcodes if total_test_opcodes > 0 else 0
            }
            
            self.logger.info("CFGæµ‹è¯•ç»“æœ (æœ€è¿‘é‚»):")
            for opcode, result in results.items():
                if opcode != '_statistics':
                    self.logger.info(f"  {opcode}: {result}")
            self.logger.info(f"  ç»Ÿè®¡: {results['_statistics']}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"CFGæµ‹è¯•å¤±è´¥: {e}")
            return {}
    
    def run_finetune(self, epochs: int = 5, lr: float = 0.05, min_count: int = 1, test_size: float = 0.2) -> bool:
        """è¿è¡Œå®Œæ•´çš„CFGè®­ç»ƒæµç¨‹"""
        try:
            self.logger.info("å¼€å§‹CFG FastTextæ¨¡å‹è®­ç»ƒæµç¨‹")
            
            # 1. é¢„å¤„ç†è¯­æ–™åº“
            texts, _ = self.preprocess_corpus()
            if not texts:
                self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„CFGè®­ç»ƒæ•°æ®")
                return False
            
            # 2. åˆ›å»ºè®­ç»ƒæ•°æ®
            train_file, val_file = self.create_training_data(texts, test_size)
            if not train_file or not val_file:
                return False
            
            # 3. è®­ç»ƒæ¨¡å‹
            if not self.finetune_model(train_file, epochs, lr, min_count):
                return False
            
            # 4. è¯„ä¼°å’Œæµ‹è¯•
            eval_results = self.evaluate_model(val_file)
            test_results = self.test_model()
            
            # 5. ä¿å­˜æŠ¥å‘Š
            self.save_report(eval_results, test_results, {'epochs': epochs, 'lr': lr, 'min_count': min_count})
            
            self.logger.info("CFG FastTextæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            return True
            
        except Exception as e:
            self.logger.error(f"CFGè®­ç»ƒæµç¨‹å¤±è´¥: {e}")
            return False
    
    def save_report(self, eval_results: dict, test_results: dict, train_params: dict = None):
        """ä¿å­˜CFGè®­ç»ƒæŠ¥å‘Š"""
        try:
            report_file = os.path.join(self.output_dir, 'cfg_finetune_report.txt')
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("CFG FastTextæ¨¡å‹è®­ç»ƒæŠ¥å‘Š\n")
                f.write("=" * 50 + "\n\n")
                
                f.write("é…ç½®ä¿¡æ¯:\n")
                f.write(f"  é¢„è®­ç»ƒæ¨¡å‹: {self.pretrained_model_path}\n")
                f.write(f"  CFGè¯­æ–™åº“: {self.corpus_path}\n")
                f.write(f"  è¾“å‡ºç›®å½•: {self.output_dir}\n\n")
                
                if train_params:
                    f.write("è®­ç»ƒå‚æ•°:\n")
                    for key, value in train_params.items():
                        f.write(f"  {key}: {value}\n")
                    f.write("\n")
                
                f.write("è¯„ä¼°ç»“æœ:\n")
                for key, value in eval_results.items():
                    f.write(f"  {key}: {value}\n")
                f.write("\n")
                
                f.write("æµ‹è¯•ç»“æœ:\n")
                for opcode, result in test_results.items():
                    f.write(f"  {opcode}: {result}\n")
            
            self.logger.info(f"CFGæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜CFGæŠ¥å‘Šå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='CFG FastTextæ¨¡å‹è®­ç»ƒå·¥å…·')
    
    parser.add_argument('--pretrained-model', 
                       default='/mnt/data1_l20_raid5disk/lbq_dataset/models/crawl-300d-2M-subword.vec',
                       help='é¢„è®­ç»ƒå‘é‡æ–‡ä»¶è·¯å¾„ (éœ€è¦.vecæ–‡ä»¶)')
    
    parser.add_argument('--corpus',
                       default='/mnt/data1_l20_raid5disk/lbq_dataset/output/cfg_corpus.txt',
                       help='CFGæ“ä½œç è¯­æ–™åº“æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--output-dir',
                       default='/mnt/data1_l20_raid5disk/lbq_dataset/output/cfg_fasttext',
                       help='è¾“å‡ºç›®å½•')
    
    parser.add_argument('--epochs', type=int, default=5, help='è®­ç»ƒè½®æ•° (é»˜è®¤: 5)')
    parser.add_argument('--lr', type=float, default=0.05, help='å­¦ä¹ ç‡ (é»˜è®¤: 0.05)')
    parser.add_argument('--min-count', type=int, default=1, help='æœ€å°è¯é¢‘ (é»˜è®¤: 1)')
    parser.add_argument('--test-size', type=float, default=0.2, help='éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.2)')
    parser.add_argument('--dim', type=int, default=300, help='å‘é‡ç»´åº¦ (é»˜è®¤: 300)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥FastText CLIå·¥å…·
    try:
        subprocess.run(['fasttext'], capture_output=True, text=True)
        print("âœ… FastText CLIå·¥å…·æ£€æµ‹æˆåŠŸ")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ é”™è¯¯: FastText CLIå·¥å…·æœªæ‰¾åˆ°")
        sys.exit(1)
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(args.pretrained_model):
        print(f"é”™è¯¯: é¢„è®­ç»ƒå‘é‡æ–‡ä»¶ä¸å­˜åœ¨: {args.pretrained_model}")
        sys.exit(1)
    
    if not os.path.exists(args.corpus):
        print(f"é”™è¯¯: CFGè¯­æ–™åº“æ–‡ä»¶ä¸å­˜åœ¨: {args.corpus}")
        sys.exit(1)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶è¿è¡Œ
    finetuner = CFGFastTextFineTuner(
        pretrained_model_path=args.pretrained_model,
        corpus_path=args.corpus,
        output_dir=args.output_dir,
        model_dim=args.dim
    )
    
    success = finetuner.run_finetune(
        epochs=args.epochs,
        lr=args.lr,
        min_count=args.min_count,
        test_size=args.test_size
    )
    
    if success:
        print("âœ… CFG FastTextæ¨¡å‹è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
        sys.exit(0)
    else:
        print("âŒ CFG FastTextæ¨¡å‹è®­ç»ƒå¤±è´¥ï¼")
        sys.exit(1)


if __name__ == "__main__":
    main()