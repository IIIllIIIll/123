#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""FastTextæ¨¡å‹è®­ç»ƒå·¥å…·

ä½¿ç”¨FastText CLIè¿›è¡ŒåŸºäºé¢„è®­ç»ƒè¯å‘é‡çš„æ¨¡å‹è®­ç»ƒã€‚

ä¸»è¦åŠŸèƒ½:
1. æ•°æ®é¢„å¤„ç†å’Œåˆ†å‰²
2. åŸºäºé¢„è®­ç»ƒè¯å‘é‡çš„FastTextæ¨¡å‹è®­ç»ƒ
3. æ¨¡å‹è¯„ä¼°å’Œæ€§èƒ½åˆ†æ
4. è®­ç»ƒæŠ¥å‘Šç”Ÿæˆ

ä½¿ç”¨æ–¹å¼:
    python fasttext_finetune.py --pretrained-model cc.en.300.vec --corpus corpus.txt --output-dir ./output

ä¾èµ–:
    - FastText C++å‘½ä»¤è¡Œå·¥å…·
    - Python fasttextåº“
    - é¢„è®­ç»ƒå‘é‡æ–‡ä»¶ï¼ˆå¦‚cc.en.300.vecï¼‰
"""

import os
import sys
import argparse
import logging
import subprocess
from typing import List, Tuple, Optional
import fasttext
import numpy as np
from sklearn.model_selection import train_test_split


class FastTextFineTuner:
    """FastTextæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, pretrained_model_path: str, corpus_path: str, output_dir: str, model_dim: int):
        """åˆå§‹åŒ–FastTextè®­ç»ƒå™¨"""
        self.pretrained_model_path = pretrained_model_path
        self.corpus_path = corpus_path
        self.output_dir = output_dir
        self.model_dim = model_dim
        
        os.makedirs(output_dir, exist_ok=True)
        self.setup_logging()
        self.model = None
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        log_file = os.path.join(self.output_dir, 'fasttext_finetune.log')
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def preprocess_corpus(self) -> Tuple[List[str], int]:
        """é¢„å¤„ç†è¯­æ–™åº“æ•°æ®"""
        try:
            self.logger.info(f"é¢„å¤„ç†è¯­æ–™åº“: {self.corpus_path}")
            
            processed_texts = []
            total_lines = 0
            
            with open(self.corpus_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # è¿‡æ»¤è¿‡çŸ­çš„å‡½æ•°å
                    functions = [func for func in line.split() if len(func) >= 3]
                    if functions:
                        processed_texts.append(' '.join(functions))
                    total_lines += 1
            
            self.logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ–‡æœ¬: {len(processed_texts)} è¡Œ")
            return processed_texts, total_lines
            
        except Exception as e:
            self.logger.error(f"é¢„å¤„ç†å¤±è´¥: {e}")
            return [], 0
    
    def create_training_data(self, texts: List[str], test_size: float = 0.2) -> Tuple[str, str]:
        """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®æ–‡ä»¶"""
        try:
            # åˆ†å‰²æ•°æ®
            if len(texts) > 1:
                train_texts, val_texts = train_test_split(texts, test_size=test_size, random_state=42)
            else:
                train_texts = texts
                val_texts = texts[:1] if texts else []
            
            # åˆ›å»ºæ–‡ä»¶
            train_file = os.path.join(self.output_dir, 'train_corpus.txt')
            val_file = os.path.join(self.output_dir, 'val_corpus.txt')
            
            with open(train_file, 'w', encoding='utf-8') as f:
                for text in train_texts:
                    f.write(text + '\n')
            
            with open(val_file, 'w', encoding='utf-8') as f:
                for text in val_texts:
                    f.write(text + '\n')
            
            self.logger.info(f"è®­ç»ƒæ•°æ®: {len(train_texts)} è¡Œï¼ŒéªŒè¯æ•°æ®: {len(val_texts)} è¡Œ")
            return train_file, val_file
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºè®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            return "", ""
    
    def finetune_model(self, train_file: str, epochs: int = 5, lr: float = 0.05, min_count: int = 1) -> bool:
        """åŸºäºé¢„è®­ç»ƒè¯å‘é‡è®­ç»ƒFastTextæ¨¡å‹"""
        try:
            self.logger.info("å¼€å§‹è®­ç»ƒFastTextæ¨¡å‹...")
            
            # å®šä¹‰è¾“å‡ºæ¨¡å‹å‰ç¼€
            output_model_prefix = os.path.join(self.output_dir, 'finetuned_fasttext_model')
            finetuned_model_path = f"{output_model_prefix}.bin"

            # æ„å»ºCLIå‘½ä»¤
            command = [
                'fasttext', 'skipgram',
                '-pretrainedVectors', self.pretrained_model_path,
                '-dim', str(self.model_dim),
                '-output', output_model_prefix,
                '-input', train_file,
                '-epoch', str(epochs),
                '-lr', str(lr),
                '-minCount', str(min_count),
                '-thread', '4',
                '-verbose', '2'
            ]
            
            # æ‰§è¡Œå‘½ä»¤
            result = subprocess.run(command, capture_output=True, text=True, encoding='utf-8')
            
            if result.returncode != 0:
                self.logger.error(f"FastText CLIæ‰§è¡Œå¤±è´¥: {result.stderr}")
                return False
            
            if not os.path.exists(finetuned_model_path):
                self.logger.error(f"æ¨¡å‹æ–‡ä»¶æœªç”Ÿæˆ: {finetuned_model_path}")
                return False

            # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
            self.model = fasttext.load_model(finetuned_model_path)
            self.logger.info(f"æ¨¡å‹è®­ç»ƒå®Œæˆ: {finetuned_model_path}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False

    def evaluate_model(self, val_file: str) -> dict:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½ (å¢å¼ºç‰ˆï¼šè®¡ç®—ç›¸é‚»ç›¸ä¼¼åº¦)"""
        try:
            if not self.model:
                self.logger.error("æ¨¡å‹æœªåŠ è½½")
                return {}
            
            self.logger.info("æ­£åœ¨è¯„ä¼°æ¨¡å‹...")
            
            # è¯»å–éªŒè¯æ•°æ®
            with open(val_file, 'r', encoding='utf-8') as f:
                val_texts = [line.strip() for line in f if line.strip()]
            
            if not val_texts:
                self.logger.warning("éªŒè¯æ•°æ®ä¸ºç©º")
                return {}
            
            # è®¡ç®—è¯æ±‡è¦†ç›–ç‡
            total_words = 0
            covered_words = 0
            all_words_set = set()
            
            for text in val_texts:
                words = text.split()
                total_words += len(words)
                all_words_set.update(words)
                for word in words:
                    if word in self.model.words:
                        covered_words += 1
            
            coverage = covered_words / total_words if total_words > 0 else 0
            
            # --- å…³é”®è¯„ä¼°ï¼šè®¡ç®—ç›¸é‚»å‡½æ•°åçš„å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ ---
            # è¿™æ˜¯ SkipGram æ¨¡å‹çš„æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡
            similarities = []
            # æŠ½æ ·è¯„ä¼°ï¼Œé¿å…è®¡ç®—è¿‡æ…¢
            sample_texts = val_texts[:min(500, len(val_texts))]
            
            for text in sample_texts:
                functions = text.split()
                if len(functions) >= 2:
                    for i in range(len(functions) - 1):
                        try:
                            # è·å–å‘é‡
                            vec1 = self.model.get_word_vector(functions[i])
                            vec2 = self.model.get_word_vector(functions[i + 1])
                            
                            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                            norm1 = np.linalg.norm(vec1)
                            norm2 = np.linalg.norm(vec2)
                            
                            if norm1 > 0 and norm2 > 0:
                                sim = np.dot(vec1, vec2) / (norm1 * norm2)
                                similarities.append(sim)
                        except:
                            # å¤„ç†å¯èƒ½çš„å¼‚å¸¸æƒ…å†µ
                            continue
            
            avg_similarity = np.mean(similarities) if similarities else 0
            
            results = {
                'vocabulary_size': len(self.model.words),
                'dimension': self.model.get_dimension(),
                'word_coverage_rate': coverage,
                'avg_adjacent_similarity': avg_similarity,  # <-- å…³é”®æŒ‡æ ‡
                'total_words_in_val': total_words,
                'total_unique_words_in_val': len(all_words_set),
                'covered_words_in_val': covered_words,
                'similarity_samples_count': len(similarities)
            }
            
            self.logger.info("è¯„ä¼°ç»“æœ:")
            for key, value in results.items():
                if isinstance(value, float):
                    self.logger.info(f"  {key}: {value:.4f}")
                else:
                    self.logger.info(f"  {key}: {value}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
            return {}
    
    def test_model(self, test_words: List[str] = None) -> dict:
        """æµ‹è¯•æ¨¡å‹åŠŸèƒ½ (å¢å¼ºç‰ˆï¼šåŒ…å«æœ€è¿‘é‚»æµ‹è¯•)"""
        try:
            if not self.model:
                self.logger.error("æ¨¡å‹æœªåŠ è½½")
                return {}
            
            self.logger.info("æ­£åœ¨æµ‹è¯•æ¨¡å‹...")
            
            # é»˜è®¤æµ‹è¯•è¯æ±‡
            if not test_words:
                test_words = [
                    'main', 'start', 'sub_401000', 'GetProcAddress', 'CreateFileA',
                    'malloc', 'free', 'printf', 'scanf', 'exit'
                ]
            
            results = {}
            oov_count = 0
            
            for word in test_words:
                try:
                    in_vocab = word in self.model.words
                    if not in_vocab:
                        oov_count += 1
                    
                    word_result = {
                        'in_vocabulary': in_vocab,
                        'vector_norm': float(np.linalg.norm(self.model.get_word_vector(word)))
                    }
                    
                    # --- å¢å¼ºåŠŸèƒ½ï¼šè·å–æœ€è¿‘é‚» ---
                    # è¿™æ˜¯è¯„ä¼°è¯å‘é‡è´¨é‡çš„é‡è¦æŒ‡æ ‡
                    if in_vocab:
                        try:
                            # è·å–æœ€è¿‘é‚»è¯æ±‡ (k=5)
                            nearest_neighbors = self.model.get_nearest_neighbors(word, k=5)
                            word_result['nearest_neighbors'] = [
                                {'word': neighbor[1], 'similarity': float(neighbor[0])}
                                for neighbor in nearest_neighbors
                            ]
                        except Exception as e:
                            word_result['nearest_neighbors_error'] = str(e)
                    else:
                        word_result['nearest_neighbors'] = []
                    
                    results[word] = word_result
                    
                except Exception as e:
                    results[word] = {'in_vocabulary': False, 'error': str(e)}
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_test_words = len(test_words)
            results['_statistics'] = {
                'total_test_words': total_test_words,
                'oov_words_count': oov_count,
                'oov_rate': oov_count / total_test_words if total_test_words > 0 else 0,
                'in_vocab_words_count': total_test_words - oov_count
            }
            
            self.logger.info(f"æµ‹è¯•å®Œæˆ - OOVç‡: {results['_statistics']['oov_rate']:.2%}")
            self.logger.info(f"è¯æ±‡è¡¨å†…è¯æ±‡æ•°: {results['_statistics']['in_vocab_words_count']}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
            return {}
    
    def run_finetune(self, epochs: int = 5, lr: float = 0.05, min_count: int = 1, test_size: float = 0.2) -> bool:
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        try:
            self.logger.info("å¼€å§‹FastTextæ¨¡å‹è®­ç»ƒæµç¨‹")
            
            # 1. é¢„å¤„ç†è¯­æ–™åº“
            texts, _ = self.preprocess_corpus()
            if not texts:
                self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
                return False
            
            # 2. åˆ›å»ºè®­ç»ƒæ•°æ®
            train_file, val_file = self.create_training_data(texts, test_size)
            if not train_file or not val_file:
                return False
            
            # 3. è®­ç»ƒæ¨¡å‹
            if not self.finetune_model(train_file, epochs, lr, min_count):
                return False
            
            # 4. è¯„ä¼°å’Œæµ‹è¯•
            eval_results = self.evaluate_model(val_file)
            test_results = self.test_model()
            
            # 5. ä¿å­˜æŠ¥å‘Š
            self.save_report(eval_results, test_results, {'epochs': epochs, 'lr': lr, 'min_count': min_count})
            
            self.logger.info("FastTextæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            return True
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒæµç¨‹å¤±è´¥: {e}")
            return False
    
    def save_report(self, eval_results: dict, test_results: dict, train_params: dict = None):
        """ä¿å­˜è®­ç»ƒæŠ¥å‘Š"""
        try:
            report_file = os.path.join(self.output_dir, 'finetune_report.txt')
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("FastTextæ¨¡å‹è®­ç»ƒæŠ¥å‘Š\n")
                f.write("=" * 50 + "\n\n")
                
                f.write("é…ç½®ä¿¡æ¯:\n")
                f.write(f"  é¢„è®­ç»ƒæ¨¡å‹: {self.pretrained_model_path}\n")
                f.write(f"  è¯­æ–™åº“: {self.corpus_path}\n")
                f.write(f"  è¾“å‡ºç›®å½•: {self.output_dir}\n\n")
                
                if train_params:
                    f.write("è®­ç»ƒå‚æ•°:\n")
                    for key, value in train_params.items():
                        f.write(f"  {key}: {value}\n")
                    f.write("\n")
                
                f.write("è¯„ä¼°ç»“æœ:\n")
                for key, value in eval_results.items():
                    f.write(f"  {key}: {value}\n")
                f.write("\n")
                
                f.write("æµ‹è¯•ç»“æœ:\n")
                for word, result in test_results.items():
                    f.write(f"  {word}: {result}\n")
            
            self.logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='FastTextæ¨¡å‹è®­ç»ƒå·¥å…·')
    
    parser.add_argument('--pretrained-model', 
                       default='/mnt/data1_l20_raid5disk/lbq_dataset/models/crawl-300d-2M-subword.vec',
                       help='é¢„è®­ç»ƒå‘é‡æ–‡ä»¶è·¯å¾„ (éœ€è¦.vecæ–‡ä»¶)')
    
    parser.add_argument('--corpus',
                       default='/mnt/data1_l20_raid5disk/lbq_dataset/output/fcg_corpus.txt',
                       help='è¯­æ–™åº“æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--output-dir',
                       default='/mnt/data1_l20_raid5disk/lbq_dataset/output/fcg_fasttext',
                       help='è¾“å‡ºç›®å½•')
    
    parser.add_argument('--epochs', type=int, default=5, help='è®­ç»ƒè½®æ•° (é»˜è®¤: 5)')
    parser.add_argument('--lr', type=float, default=0.05, help='å­¦ä¹ ç‡ (é»˜è®¤: 0.05)')
    parser.add_argument('--min-count', type=int, default=1, help='æœ€å°è¯é¢‘ (é»˜è®¤: 1)')
    parser.add_argument('--test-size', type=float, default=0.2, help='éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.2)')
    parser.add_argument('--dim', type=int, default=300, help='å‘é‡ç»´åº¦ (é»˜è®¤: 300)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥FastText CLIå·¥å…·
    try:
        subprocess.run(['fasttext'], capture_output=True, text=True)
        print("âœ… FastText CLIå·¥å…·æ£€æµ‹æˆåŠŸ")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ é”™è¯¯: FastText CLIå·¥å…·æœªæ‰¾åˆ°")
        sys.exit(1)
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(args.pretrained_model):
        print(f"é”™è¯¯: é¢„è®­ç»ƒå‘é‡æ–‡ä»¶ä¸å­˜åœ¨: {args.pretrained_model}")
        sys.exit(1)
    
    if not os.path.exists(args.corpus):
        print(f"é”™è¯¯: è¯­æ–™åº“æ–‡ä»¶ä¸å­˜åœ¨: {args.corpus}")
        sys.exit(1)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶è¿è¡Œ
    finetuner = FastTextFineTuner(
        pretrained_model_path=args.pretrained_model,
        corpus_path=args.corpus,
        output_dir=args.output_dir,
        model_dim=args.dim
    )
    
    success = finetuner.run_finetune(
        epochs=args.epochs,
        lr=args.lr,
        min_count=args.min_count,
        test_size=args.test_size
    )
    
    if success:
        print("âœ… FastTextæ¨¡å‹è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
        sys.exit(0)
    else:
        print("âŒ FastTextæ¨¡å‹è®­ç»ƒå¤±è´¥ï¼")
        sys.exit(1)


if __name__ == "__main__":
    main()