import torch.nn.functional as F

def distinctiveness_loss(expert_outputs):
    """
    计算专家之间的多样性损失（Distinctiveness Loss），
    通过计算输出之间的相似性（如高斯核），最小化专家输出的相似性。
    
    参数:
    expert_outputs: list of tensors, 每个专家的输出
    """
    loss = 0
    num_experts = len(expert_outputs)
    
    for i in range(num_experts):
        for j in range(i + 1, num_experts):
            # 计算两个专家输出的相似性
            expert_i = expert_outputs[i]
            expert_j = expert_outputs[j]
            
            # 使用高斯核计算相似性
            similarity = torch.exp(-torch.norm(expert_i - expert_j, dim=1) ** 2 / 2)
            loss += similarity.mean()
    
    return loss

def equilibrium_loss(expert_weights):
    """
    计算专家平衡损失（Equilibrium Loss），
    通过计算每个专家的选择频率，鼓励均衡使用所有专家。
    
    参数:
    expert_weights: Tensor, 每个专家的选择权重
    """
    expert_usage = expert_weights.sum(dim=0) / expert_weights.size(0)  # 计算每个专家的使用频率
    expected_usage = torch.ones_like(expert_usage) / expert_usage.size(0)  # 假设均衡使用的目标是均匀的
    
    # 使用L2范数计算专家选择的均衡程度
    loss = torch.norm(expert_usage - expected_usage, p=2)
    return loss


class MixtureOfExperts(nn.Module):
    def __init__(self, input_dim=128, hidden_dim=64, output_dim=64, num_experts=None, k=None):
        super(MixtureOfExperts, self).__init__()
        self.num_experts = num_experts
        self.k = k
        
        self.experts = nn.ModuleList([
            ExpertNetwork(input_dim, hidden_dim, output_dim) 
            for _ in range(self.num_experts)
        ])
        
        self.gating = GatingNetwork(input_dim, self.num_experts)
        
    def forward(self, x):
        gates = self.gating(x)
        _, indices = torch.topk(gates, self.k, dim=1)
        final_output = torch.zeros(x.size(0), self.experts[0](x).size(1), device=x.device)
        
        expert_outputs = []
        
        for i in range(x.size(0)):
            sample_output = 0
            expert_indices = indices[i]
            expert_weights = gates[i, expert_indices]
            expert_weights = expert_weights / expert_weights.sum()
            
            for j, expert_idx in enumerate(expert_indices):
                expert_out = self.experts[expert_idx](x[i:i+1])
                sample_output += expert_out * expert_weights[j]
                expert_outputs.append(expert_out)  # 保存每个专家的输出供损失计算使用
            
            final_output[i] = sample_output
        
        # 计算多样性损失和均衡损失
        distinct_loss = distinctiveness_loss(expert_outputs)
        eq_loss = equilibrium_loss(gates)
        
        return final_output, distinct_loss, eq_loss


# 在训练过程中调用新的损失函数
for vec_binary, vec_cfg, vec_fcg, labels in tqdm(train_loader, desc=f"Epoch {epoch+1} Train"):
    vec_binary = vec_binary.to(device)
    vec_cfg = vec_cfg.to(device)
    vec_fcg = vec_fcg.to(device)
    labels = labels.to(device)
    
    optimizer.zero_grad()
    
    # 计算MoE的输出和损失
    binary_out, binary_dist_loss, binary_eq_loss = model.shared_moe(vec_binary)
    cfg_out, cfg_dist_loss, cfg_eq_loss = model.shared_moe(vec_cfg)
    fcg_out, fcg_dist_loss, fcg_eq_loss = model.shared_moe(vec_fcg)
    
    # 合并输出并进行分类
    combined = torch.cat([binary_out, cfg_out, fcg_out], dim=1)
    fused = F.relu(model.fusion(combined))
    output = model.classifier(fused)
    
    # 计算分类损失
    classification_loss = criterion(output, labels)
    
    # 总损失包括分类损失、平衡损失和多样性损失
    total_loss = classification_loss + binary_dist_loss + binary_eq_loss + cfg_dist_loss + cfg_eq_loss + fcg_dist_loss + fcg_eq_loss
    total_loss.backward()
    optimizer.step()

    
# 通过超参数调整平衡损失和多样性损失的权重
lambda_eq = 0.1  # 平衡损失的权重
lambda_dist = 0.1  # 多样性损失的权重

total_loss = classification_loss + lambda_eq * (binary_eq_loss + cfg_eq_loss + fcg_eq_loss) + lambda_dist * (binary_dist_loss + cfg_dist_loss + fcg_dist_loss)
